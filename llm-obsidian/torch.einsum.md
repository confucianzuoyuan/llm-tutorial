# torch.einsum å®Œå…¨æŒ‡å—

## ä¸€ã€ä»€ä¹ˆæ˜¯einsumï¼Ÿ

### åŸºæœ¬æ¦‚å¿µ

**einsum = Einstein Summationï¼ˆçˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šï¼‰**

è¿™æ˜¯ä¸€ç§ç”¨ç®€æ´ç¬¦å·è¡¨ç¤ºå¼ é‡è¿ç®—çš„æ–¹æ³•ï¼Œç”±çˆ±å› æ–¯å¦åœ¨ç›¸å¯¹è®ºä¸­æå‡ºã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š**
- ç”¨å­—æ¯è¡¨ç¤ºå¼ é‡çš„ç»´åº¦
- é‡å¤çš„å­—æ¯è¡¨ç¤ºæ±‚å’Œ
- ä¸å‡ºç°åœ¨è¾“å‡ºçš„å­—æ¯è¡¨ç¤ºè¢«æ±‚å’Œæ‰

---

### è¯­æ³•

```python
torch.einsum(equation, *operands)
```

**å‚æ•°ï¼š**
- `equation`ï¼šå­—ç¬¦ä¸²ï¼Œæè¿°è¿ç®—è§„åˆ™
- `*operands`ï¼šè¾“å…¥å¼ é‡

**æ ¼å¼ï¼š**
```
"è¾“å…¥1ç´¢å¼•,è¾“å…¥2ç´¢å¼•,...->è¾“å‡ºç´¢å¼•"
```

---

## äºŒã€åŸºç¡€è§„åˆ™

### è§„åˆ™1ï¼šç»´åº¦æ ‡è®°

```python
import torch

# ä¸€ä¸ªçŸ©é˜µ [3, 4]
A = torch.randn(3, 4)

# ç”¨ 'ij' è¡¨ç¤ºï¼šiæ˜¯ç¬¬0ç»´(3), jæ˜¯ç¬¬1ç»´(4)
# i å¯¹åº”è¡Œï¼Œj å¯¹åº”åˆ—
```

**å­—æ¯çš„å«ä¹‰ï¼š**
- æ¯ä¸ªå­—æ¯ä»£è¡¨ä¸€ä¸ªç»´åº¦
- å­—æ¯çš„é¡ºåºå¯¹åº”å¼ é‡çš„ç»´åº¦é¡ºåº
- é€šå¸¸ç”¨ i, j, k, l, m, n...

---

### è§„åˆ™2ï¼šé‡å¤å­—æ¯ = æ±‚å’Œ

```python
# 'ii' è¡¨ç¤ºå¯¹è§’çº¿å…ƒç´ æ±‚å’Œï¼ˆtraceï¼‰
A = torch.tensor([[1, 2],
                  [3, 4]])

result = torch.einsum('ii', A)
# ç­‰ä»·äºï¼šA[0,0] + A[1,1] = 1 + 4 = 5
print(result)  # tensor(5)
```

---

### è§„åˆ™3ï¼šç®­å¤´å³è¾¹ = è¾“å‡ºç»´åº¦

```python
# 'ij->ji' è¡¨ç¤ºè½¬ç½®
A = torch.randn(3, 4)
result = torch.einsum('ij->ji', A)
# result.shape = [4, 3]
```

---

### è§„åˆ™4ï¼šçœç•¥ç®­å¤´ = è‡ªåŠ¨æ¨æ–­

```python
# çœç•¥ '->' æ—¶ï¼Œè¾“å‡ºåŒ…å«æ‰€æœ‰åªå‡ºç°ä¸€æ¬¡çš„å­—æ¯ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
A = torch.randn(3, 4)
result = torch.einsum('ij', A)
# ç­‰ä»·äº 'ij->ij'ï¼Œå³è¿”å›åŸçŸ©é˜µ
```

---

## ä¸‰ã€å¸¸è§æ“ä½œç¤ºä¾‹

### 1. è½¬ç½®ï¼ˆTransposeï¼‰

```python
A = torch.randn(3, 4)

# æ–¹æ³•1ï¼šeinsum
result = torch.einsum('ij->ji', A)

# æ–¹æ³•2ï¼šä¼ ç»Ÿ
result = A.T

print(result.shape)  # [4, 3]
```

**è§£é‡Šï¼š**
```
è¾“å…¥ï¼š'ij' è¡¨ç¤º A[i, j]
è¾“å‡ºï¼š'ji' è¡¨ç¤ºç»“æœ[j, i]
â†’ è¡Œåˆ—äº’æ¢
```

---

### 2. æ±‚å’Œï¼ˆSumï¼‰

#### å…¨éƒ¨æ±‚å’Œ

```python
A = torch.tensor([[1, 2, 3],
                  [4, 5, 6]])

# einsumï¼šä¸æŒ‡å®šè¾“å‡ºç»´åº¦ = å…¨éƒ¨æ±‚å’Œ
result = torch.einsum('ij->', A)
# ç­‰ä»·äºï¼šA.sum()

print(result)  # tensor(21) = 1+2+3+4+5+6
```

#### æŒ‰è¡Œæ±‚å’Œ

```python
# 'ij->i' ä¿ç•™iç»´åº¦ï¼Œå¯¹jæ±‚å’Œ
result = torch.einsum('ij->i', A)
# ç­‰ä»·äºï¼šA.sum(dim=1)

print(result)  # tensor([6, 15])
# 6 = 1+2+3
# 15 = 4+5+6
```

#### æŒ‰åˆ—æ±‚å’Œ

```python
# 'ij->j' ä¿ç•™jç»´åº¦ï¼Œå¯¹iæ±‚å’Œ
result = torch.einsum('ij->j', A)
# ç­‰ä»·äºï¼šA.sum(dim=0)

print(result)  # tensor([5, 7, 9])
# 5 = 1+4
# 7 = 2+5
# 9 = 3+6
```

---

### 3. å¯¹è§’çº¿ï¼ˆDiagonalï¼‰

```python
A = torch.tensor([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])

# æå–å¯¹è§’çº¿
result = torch.einsum('ii->i', A)
# ç­‰ä»·äºï¼štorch.diag(A)

print(result)  # tensor([1, 5, 9])
```

**æ³¨æ„ï¼šè¿™é‡Œè¾“å…¥æ˜¯'ii'ï¼ˆä¸¤ä¸ªiï¼‰ï¼Œä½†è¾“å‡ºæ˜¯'i'ï¼ˆä¸€ä¸ªiï¼‰**

---

### 4. è¿¹ï¼ˆTraceï¼‰

```python
A = torch.tensor([[1, 2],
                  [3, 4]])

# å¯¹è§’çº¿å…ƒç´ æ±‚å’Œ
result = torch.einsum('ii->', A)
# ç­‰ä»·äºï¼štorch.trace(A)

print(result)  # tensor(5) = 1+4
```

---

### 5. çŸ©é˜µä¹˜æ³•ï¼ˆMatrix Multiplicationï¼‰

```python
A = torch.randn(3, 4)  # [3, 4]
B = torch.randn(4, 5)  # [4, 5]

# einsum
result = torch.einsum('ik,kj->ij', A, B)
# ç­‰ä»·äºï¼šA @ B

print(result.shape)  # [3, 5]
```

**è¯¦ç»†è§£é‡Šï¼š**

$$
C_{ij} = \sum_{k} A_{ik} B_{kj}
$$

```
'ik,kj->ij'
 â†“   â†“   â†“
 A   B   C

- Açš„ç»´åº¦ï¼ši(3), k(4)
- Bçš„ç»´åº¦ï¼šk(4), j(5)
- ké‡å¤ â†’ å¯¹kæ±‚å’Œ
- è¾“å‡ºï¼ši(3), j(5)
```

---

### 6. æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆBatch Matrix Multiplicationï¼‰

```python
A = torch.randn(10, 3, 4)  # [batch, 3, 4]
B = torch.randn(10, 4, 5)  # [batch, 4, 5]

# einsum
result = torch.einsum('bik,bkj->bij', A, B)
# ç­‰ä»·äºï¼štorch.bmm(A, B)

print(result.shape)  # [10, 3, 5]
```

**è§£é‡Šï¼š**
```
'bik,bkj->bij'
 â†“    â†“    â†“
 A    B    C

- bï¼šbatchç»´åº¦ï¼ˆä¸æ±‚å’Œï¼Œä¿ç•™ï¼‰
- i, jï¼šçŸ©é˜µç»´åº¦ï¼ˆä¿ç•™ï¼‰
- kï¼šé‡å¤ â†’ æ±‚å’Œ
```

---

### 7. å‘é‡ç‚¹ç§¯ï¼ˆDot Productï¼‰

```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# einsum
result = torch.einsum('i,i->', a, b)
# ç­‰ä»·äºï¼štorch.dot(a, b)

print(result)  # tensor(32) = 1*4 + 2*5 + 3*6
```

**è§£é‡Šï¼š**
```
'i,i->'
 â†“ â†“  â†“
 a b  æ ‡é‡

- ié‡å¤ä¸¤æ¬¡ â†’ å¯¹iæ±‚å’Œ
- è¾“å‡ºä¸ºæ ‡é‡ï¼ˆæ— ç»´åº¦ï¼‰
```

---

### 8. å¤–ç§¯ï¼ˆOuter Productï¼‰

```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5])

# einsum
result = torch.einsum('i,j->ij', a, b)
# ç­‰ä»·äºï¼štorch.outer(a, b)

print(result)
# tensor([[ 4,  5],
#         [ 8, 10],
#         [12, 15]])
```

**è§£é‡Šï¼š**

$$
C_{ij} = a_i \times b_j
$$

```
'i,j->ij'
 â†“ â†“  â†“
 a b  çŸ©é˜µ

- i, j éƒ½ä¸é‡å¤ â†’ ä¸æ±‚å’Œ
- è¾“å‡ºï¼šiÃ—j çš„çŸ©é˜µ
```

---

### 9. æ‰¹é‡ç‚¹ç§¯ï¼ˆBatch Dot Productï¼‰

```python
A = torch.randn(10, 3)  # [batch, dim]
B = torch.randn(10, 3)  # [batch, dim]

# æ¯ä¸ªæ ·æœ¬è®¡ç®—ç‚¹ç§¯
result = torch.einsum('bi,bi->b', A, B)

print(result.shape)  # [10]
```

**è§£é‡Šï¼š**
```
'bi,bi->b'
 â†“   â†“  â†“
 A   B  å‘é‡

- bï¼šbatchç»´åº¦ï¼ˆä¿ç•™ï¼‰
- iï¼šé‡å¤ â†’ å¯¹iæ±‚å’Œ
- è¾“å‡ºï¼šé•¿åº¦ä¸ºbatchçš„å‘é‡
```

---

### 10. Hadamardç§¯ï¼ˆé€å…ƒç´ ä¹˜æ³•ï¼‰

```python
A = torch.randn(3, 4)
B = torch.randn(3, 4)

# einsum
result = torch.einsum('ij,ij->ij', A, B)
# ç­‰ä»·äºï¼šA * B

print(result.shape)  # [3, 4]
```

---

## å››ã€é«˜çº§åº”ç”¨

### 1. æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰

```python
# Q: [batch, seq_len, d_k]
# K: [batch, seq_len, d_k]
# V: [batch, seq_len, d_v]

Q = torch.randn(2, 10, 64)  # [batch, seq_q, d_k]
K = torch.randn(2, 20, 64)  # [batch, seq_k, d_k]
V = torch.randn(2, 20, 128) # [batch, seq_k, d_v]

# è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ @ K.T
scores = torch.einsum('bqd,bkd->bqk', Q, K)
# scores.shape = [2, 10, 20]

# ç­‰ä»·äºï¼š
# scores = torch.matmul(Q, K.transpose(-2, -1))

# æ³¨æ„åŠ›åŠ æƒ
attention = torch.softmax(scores, dim=-1)
output = torch.einsum('bqk,bkv->bqv', attention, V)
# output.shape = [2, 10, 128]
```

**å®Œæ•´çš„Attentionï¼š**

```python
def attention_einsum(Q, K, V):
    """
    Q: [batch, seq_q, d_k]
    K: [batch, seq_k, d_k]
    V: [batch, seq_k, d_v]
    """
    d_k = Q.shape[-1]
    
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.einsum('bqd,bkd->bqk', Q, K) / (d_k ** 0.5)
    
    # Softmax
    attention = torch.softmax(scores, dim=-1)
    
    # åŠ æƒæ±‚å’Œ
    output = torch.einsum('bqk,bkv->bqv', attention, V)
    
    return output, attention

# æµ‹è¯•
Q = torch.randn(2, 10, 64)
K = torch.randn(2, 20, 64)
V = torch.randn(2, 20, 128)

output, attn = attention_einsum(Q, K, V)
print(output.shape)  # [2, 10, 128]
print(attn.shape)    # [2, 10, 20]
```

---

### 2. å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

```python
# Q, K, V: [batch, num_heads, seq_len, d_k]
Q = torch.randn(2, 8, 10, 64)  # [batch, heads, seq, d_k]
K = torch.randn(2, 8, 20, 64)
V = torch.randn(2, 8, 20, 64)

# è®¡ç®—æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›åˆ†æ•°
scores = torch.einsum('bhqd,bhkd->bhqk', Q, K)
# scores.shape = [2, 8, 10, 20]

attention = torch.softmax(scores, dim=-1)
output = torch.einsum('bhqk,bhkv->bhqv', attention, V)
# output.shape = [2, 8, 10, 64]
```

---

### 3. åŒçº¿æ€§å±‚ï¼ˆBilinear Layerï¼‰

```python
# åŒçº¿æ€§å˜æ¢ï¼šy = x1^T W x2

x1 = torch.randn(32, 10)  # [batch, dim1]
x2 = torch.randn(32, 20)  # [batch, dim2]
W = torch.randn(10, 20)   # [dim1, dim2]

# einsumå®ç°
result = torch.einsum('bi,ij,bj->b', x1, W, x2)
# result.shape = [32]

# ç­‰ä»·äºï¼š
# result = (x1 @ W * x2).sum(dim=1)
```

---

### 4. å¼ é‡æ”¶ç¼©ï¼ˆTensor Contractionï¼‰

```python
# å››ç»´å¼ é‡çš„å¤æ‚è¿ç®—
A = torch.randn(3, 4, 5, 6)
B = torch.randn(5, 6, 7, 8)

# å¯¹ç»´åº¦2å’Œ3æ”¶ç¼©
result = torch.einsum('ijkl,klmn->ijmn', A, B)
# result.shape = [3, 4, 7, 8]
```

**è§£é‡Šï¼š**
```
A: [i=3, j=4, k=5, l=6]
B: [k=5, l=6, m=7, n=8]

k, l é‡å¤ â†’ æ±‚å’Œ
è¾“å‡ºï¼š[i=3, j=4, m=7, n=8]
```

---

### 5. æ‰¹é‡å¤–ç§¯

```python
# å¯¹batchä¸­çš„æ¯ä¸ªæ ·æœ¬è®¡ç®—å¤–ç§¯
A = torch.randn(10, 3)  # [batch, dim1]
B = torch.randn(10, 4)  # [batch, dim2]

result = torch.einsum('bi,bj->bij', A, B)
# result.shape = [10, 3, 4]

print(result[0])  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å¤–ç§¯
```

---

### 6. åæ–¹å·®çŸ©é˜µ

```python
# X: [n_samples, n_features]
X = torch.randn(100, 10)

# ä¸­å¿ƒåŒ–
X_centered = X - X.mean(dim=0, keepdim=True)

# åæ–¹å·®çŸ©é˜µ
cov = torch.einsum('ni,nj->ij', X_centered, X_centered) / (X.shape[0] - 1)
# cov.shape = [10, 10]

# ç­‰ä»·äºï¼š
# cov = (X_centered.T @ X_centered) / (X.shape[0] - 1)
```

---

## äº”ã€å®Œæ•´çš„å¯¹æ¯”ç¤ºä¾‹

```python
import torch
import time

def compare_operations():
    """å¯¹æ¯”einsumå’Œä¼ ç»Ÿæ–¹æ³•"""
    
    print("="*60)
    print("einsum vs ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”")
    print("="*60)
    
    # 1. çŸ©é˜µä¹˜æ³•
    print("\n1. çŸ©é˜µä¹˜æ³•")
    A = torch.randn(1000, 500)
    B = torch.randn(500, 300)
    
    # einsum
    start = time.time()
    result1 = torch.einsum('ik,kj->ij', A, B)
    time1 = time.time() - start
    
    # ä¼ ç»Ÿ
    start = time.time()
    result2 = A @ B
    time2 = time.time() - start
    
    print(f"  einsum: {time1:.6f}s")
    print(f"  ä¼ ç»Ÿ:   {time2:.6f}s")
    print(f"  å·®å¼‚:   {torch.abs(result1 - result2).max().item():.2e}")
    
    # 2. æ‰¹é‡çŸ©é˜µä¹˜æ³•
    print("\n2. æ‰¹é‡çŸ©é˜µä¹˜æ³•")
    A = torch.randn(32, 100, 50)
    B = torch.randn(32, 50, 30)
    
    # einsum
    start = time.time()
    result1 = torch.einsum('bik,bkj->bij', A, B)
    time1 = time.time() - start
    
    # ä¼ ç»Ÿ
    start = time.time()
    result2 = torch.bmm(A, B)
    time2 = time.time() - start
    
    print(f"  einsum: {time1:.6f}s")
    print(f"  ä¼ ç»Ÿ:   {time2:.6f}s")
    print(f"  å·®å¼‚:   {torch.abs(result1 - result2).max().item():.2e}")
    
    # 3. æ³¨æ„åŠ›è®¡ç®—
    print("\n3. æ³¨æ„åŠ›è®¡ç®—")
    Q = torch.randn(32, 10, 64)
    K = torch.randn(32, 20, 64)
    V = torch.randn(32, 20, 128)
    
    # einsum
    start = time.time()
    scores1 = torch.einsum('bqd,bkd->bqk', Q, K)
    attn1 = torch.softmax(scores1, dim=-1)
    output1 = torch.einsum('bqk,bkv->bqv', attn1, V)
    time1 = time.time() - start
    
    # ä¼ ç»Ÿ
    start = time.time()
    scores2 = torch.matmul(Q, K.transpose(-2, -1))
    attn2 = torch.softmax(scores2, dim=-1)
    output2 = torch.matmul(attn2, V)
    time2 = time.time() - start
    
    print(f"  einsum: {time1:.6f}s")
    print(f"  ä¼ ç»Ÿ:   {time2:.6f}s")
    print(f"  å·®å¼‚:   {torch.abs(output1 - output2).max().item():.2e}")
    
    # 4. å¤æ‚å¼ é‡è¿ç®—
    print("\n4. å››ç»´å¼ é‡æ”¶ç¼©")
    A = torch.randn(10, 20, 30, 40)
    B = torch.randn(30, 40, 50, 60)
    
    # einsum
    start = time.time()
    result1 = torch.einsum('ijkl,klmn->ijmn', A, B)
    time1 = time.time() - start
    
    # ä¼ ç»Ÿï¼ˆéœ€è¦reshapeå’Œå¤šæ¬¡æ“ä½œï¼‰
    start = time.time()
    A_reshaped = A.reshape(10*20, 30*40)
    B_reshaped = B.reshape(30*40, 50*60)
    result2_temp = A_reshaped @ B_reshaped
    result2 = result2_temp.reshape(10, 20, 50, 60)
    time2 = time.time() - start
    
    print(f"  einsum: {time1:.6f}s")
    print(f"  ä¼ ç»Ÿ:   {time2:.6f}s")
    print(f"  å·®å¼‚:   {torch.abs(result1 - result2).max().item():.2e}")


def einsum_cheatsheet():
    """einsumé€ŸæŸ¥è¡¨"""
    
    print("\n" + "="*60)
    print("einsumé€ŸæŸ¥è¡¨")
    print("="*60)
    
    examples = [
        ("è½¬ç½®", "ij->ji", "A.T"),
        ("æ±‚å’Œ", "ij->", "A.sum()"),
        ("æŒ‰è¡Œæ±‚å’Œ", "ij->i", "A.sum(dim=1)"),
        ("æŒ‰åˆ—æ±‚å’Œ", "ij->j", "A.sum(dim=0)"),
        ("å¯¹è§’çº¿", "ii->i", "torch.diag(A)"),
        ("è¿¹", "ii->", "torch.trace(A)"),
        ("çŸ©é˜µä¹˜æ³•", "ik,kj->ij", "A @ B"),
        ("æ‰¹é‡çŸ©é˜µä¹˜æ³•", "bik,bkj->bij", "torch.bmm(A, B)"),
        ("å‘é‡ç‚¹ç§¯", "i,i->", "torch.dot(a, b)"),
        ("å¤–ç§¯", "i,j->ij", "torch.outer(a, b)"),
        ("é€å…ƒç´ ä¹˜æ³•", "ij,ij->ij", "A * B"),
        ("æ‰¹é‡ç‚¹ç§¯", "bi,bi->b", "(A * B).sum(dim=1)"),
    ]
    
    print(f"\n{'æ“ä½œ':<15} {'einsum':<20} {'ç­‰ä»·æ–¹æ³•':<20}")
    print("-"*60)
    for name, einsum_str, equiv in examples:
        print(f"{name:<15} {einsum_str:<20} {equiv:<20}")


# è¿è¡Œ
if __name__ == "__main__":
    compare_operations()
    einsum_cheatsheet()
```

---

## å…­ã€einsumçš„ä¼˜ç¼ºç‚¹

### ä¼˜ç‚¹ âœ…

**1. ç®€æ´è¡¨è¾¾**
```python
# å¤æ‚çš„å¼ é‡è¿ç®—ä¸€è¡Œæå®š
result = torch.einsum('bqhd,bkhd->bhqk', Q, K)

# ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤šæ­¥
Q_reshaped = Q.permute(0, 2, 1, 3)
K_reshaped = K.permute(0, 2, 3, 1)
result = torch.matmul(Q_reshaped, K_reshaped)
```

**2. å¯è¯»æ€§å¼º**
```python
# 'bik,bkj->bij' æ¸…æ¥šåœ°è¡¨è¾¾äº†ç»´åº¦å…³ç³»
# ä¸€çœ¼å°±èƒ½çœ‹å‡ºå“ªäº›ç»´åº¦æ±‚å’Œï¼Œå“ªäº›ä¿ç•™
```

**3. çµæ´»æ€§é«˜**
```python
# å¯ä»¥å¤„ç†ä»»æ„ç»´åº¦çš„å¼ é‡
# ä¸éœ€è¦æ‰‹åŠ¨reshapeå’Œpermute
```

**4. è‡ªåŠ¨ä¼˜åŒ–**
```python
# PyTorchä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„è®¡ç®—è·¯å¾„
# å¯¹äºå¤æ‚è¿ç®—ï¼Œå¯èƒ½æ¯”æ‰‹å†™æ›´å¿«
```

---

### ç¼ºç‚¹ âŒ

**1. å­¦ä¹ æ›²çº¿**
```python
# åˆå­¦è€…éœ€è¦æ—¶é—´ç†è§£ç¬¦å·å«ä¹‰
# ä¸å¦‚ä¼ ç»Ÿæ–¹æ³•ç›´è§‚
```

**2. è°ƒè¯•å›°éš¾**
```python
# å‡ºé”™æ—¶ï¼Œé”™è¯¯ä¿¡æ¯å¯èƒ½ä¸å¤Ÿæ˜ç¡®
# 'ijk,jkl->il' å†™é”™äº†ä¸å®¹æ˜“å‘ç°
```

**3. æ€§èƒ½ä¸ç¨³å®š**
```python
# ç®€å•æ“ä½œå¯èƒ½æ¯”ä¼ ç»Ÿæ–¹æ³•æ…¢
# éœ€è¦æµ‹è¯•æ‰çŸ¥é“æ˜¯å¦æ›´å¿«
```

**4. ä¸æ”¯æŒæ‰€æœ‰æ“ä½œ**
```python
# åªèƒ½è¡¨è¾¾çº¿æ€§ä»£æ•°è¿ç®—
# ä¸èƒ½è¡¨è¾¾æ¡ä»¶ã€å¾ªç¯ç­‰
```

---

## ä¸ƒã€ä½¿ç”¨å»ºè®®

### ä½•æ—¶ä½¿ç”¨einsumï¼Ÿ

**âœ… æ¨èä½¿ç”¨ï¼š**
```python
# 1. å¤æ‚çš„å¼ é‡æ”¶ç¼©
torch.einsum('bijk,bjkl->bil', A, B)

# 2. å¤šç»´æ‰¹é‡è¿ç®—
torch.einsum('bhqd,bhkd->bhqk', Q, K)

# 3. éœ€è¦æ¸…æ™°è¡¨è¾¾ç»´åº¦å…³ç³»
torch.einsum('bni,bnj->bij', X, X)

# 4. å®ç°è®ºæ–‡ä¸­çš„å…¬å¼
# è®ºæ–‡ï¼šC_ij = Î£_k A_ik B_kj
torch.einsum('ik,kj->ij', A, B)
```

**âŒ ä¸æ¨èä½¿ç”¨ï¼š**
```python
# 1. ç®€å•çš„çŸ©é˜µä¹˜æ³•
A @ B  # æ¯” einsum('ik,kj->ij', A, B) æ›´æ¸…æ™°

# 2. ç®€å•çš„æ±‚å’Œ
A.sum()  # æ¯” einsum('ij->', A) æ›´ç›´è§‚

# 3. è½¬ç½®
A.T  # æ¯” einsum('ij->ji', A) æ›´ç®€æ´
```

---

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# 1. å¯¹äºç®€å•æ“ä½œï¼Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ›´å¿«
# æµ‹è¯•å¯¹æ¯”ï¼š
import time

A = torch.randn(1000, 1000)
B = torch.randn(1000, 1000)

# einsum
start = time.time()
result1 = torch.einsum('ik,kj->ij', A, B)
print(f"einsum: {time.time() - start:.6f}s")

# ä¼ ç»Ÿ
start = time.time()
result2 = A @ B
print(f"ä¼ ç»Ÿ: {time.time() - start:.6f}s")

# 2. ä½¿ç”¨opt_einsumåº“è¿›ä¸€æ­¥ä¼˜åŒ–
# pip install opt_einsum
import opt_einsum as oe

# è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜è®¡ç®—è·¯å¾„
result = oe.contract('ijk,jkl,lmn->imn', A, B, C, backend='torch')
```

---

## å…«ã€å¸¸è§é”™è¯¯

### é”™è¯¯1ï¼šç»´åº¦ä¸åŒ¹é…

```python
A = torch.randn(3, 4)
B = torch.randn(5, 6)

# é”™è¯¯ï¼
try:
    result = torch.einsum('ik,kj->ij', A, B)
except RuntimeError as e:
    print(f"é”™è¯¯: {e}")
    # kåœ¨Aä¸­æ˜¯4ï¼Œåœ¨Bä¸­æ˜¯5ï¼Œä¸åŒ¹é…ï¼

# æ­£ç¡®
B = torch.randn(4, 6)
result = torch.einsum('ik,kj->ij', A, B)  # âœ“
```

---

### é”™è¯¯2ï¼šå­—æ¯é‡å¤ä½¿ç”¨ä¸å½“

```python
A = torch.randn(3, 4)
B = torch.randn(3, 4)

# é”™è¯¯ï¼šæƒ³åšé€å…ƒç´ ä¹˜æ³•ï¼Œä½†å†™æˆäº†æ±‚å’Œ
result = torch.einsum('ij,ij->', A, B)
# è¿™ä¼šå¯¹æ‰€æœ‰å…ƒç´ æ±‚å’Œï¼

# æ­£ç¡®
result = torch.einsum('ij,ij->ij', A, B)  # é€å…ƒç´ ä¹˜æ³•
```

---

### é”™è¯¯3ï¼šå¿˜è®°æŒ‡å®šè¾“å‡ºç»´åº¦

```python
A = torch.randn(3, 4)

# ä¸æŒ‡å®šè¾“å‡º
result = torch.einsum('ij', A)
# ç­‰ä»·äº 'ij->ij'ï¼Œè¿”å›åŸçŸ©é˜µ

# å¦‚æœæƒ³æ±‚å’Œï¼Œéœ€è¦æ˜ç¡®æŒ‡å®š
result = torch.einsum('ij->', A)  # å…¨éƒ¨æ±‚å’Œ
```

---

## ä¹ã€æ€»ç»“

### æ ¸å¿ƒè§„åˆ™

```
1. å­—æ¯è¡¨ç¤ºç»´åº¦
   'ij' â†’ äºŒç»´å¼ é‡ï¼Œiæ˜¯ç¬¬0ç»´ï¼Œjæ˜¯ç¬¬1ç»´

2. é‡å¤å­—æ¯ = æ±‚å’Œ
   'ik,kj' â†’ ké‡å¤ï¼Œå¯¹kæ±‚å’Œ

3. è¾“å‡ºåªåŒ…å«æŒ‡å®šçš„å­—æ¯
   'ik,kj->ij' â†’ è¾“å‡ºåªæœ‰iå’Œj

4. çœç•¥ç®­å¤´ = è‡ªåŠ¨æ¨æ–­
   'ij' â†’ 'ij->ij'ï¼ˆè¿”å›åŸå¼ é‡ï¼‰
```

---

### å¸¸ç”¨æ¨¡å¼é€ŸæŸ¥

| æ“ä½œ | einsum | å½¢çŠ¶ç¤ºä¾‹ |
|------|--------|----------|
| è½¬ç½® | `'ij->ji'` | [3,4]â†’[4,3] |
| æ±‚å’Œ | `'ij->'` | [3,4]â†’[] |
| æŒ‰è¡Œæ±‚å’Œ | `'ij->i'` | [3,4]â†’[3] |
| å¯¹è§’çº¿ | `'ii->i'` | [3,3]â†’[3] |
| è¿¹ | `'ii->'` | [3,3]â†’[] |
| çŸ©é˜µä¹˜æ³• | `'ik,kj->ij'` | [3,4],[4,5]â†’[3,5] |
| æ‰¹é‡çŸ©é˜µä¹˜æ³• | `'bik,bkj->bij'` | [B,3,4],[B,4,5]â†’[B,3,5] |
| ç‚¹ç§¯ | `'i,i->'` | [3],[3]â†’[] |
| å¤–ç§¯ | `'i,j->ij'` | [3],[4]â†’[3,4] |
| é€å…ƒç´ ä¹˜ | `'ij,ij->ij'` | [3,4],[3,4]â†’[3,4] |

---

### è®°å¿†è¦ç‚¹

```
einsum = "ç”¨å­—æ¯æè¿°å¼ é‡è¿ç®—"

è§„åˆ™ï¼š
1. æ¯ä¸ªå­—æ¯ = ä¸€ä¸ªç»´åº¦
2. é‡å¤å­—æ¯ = æ±‚å’Œç»´åº¦
3. è¾“å‡ºå­—æ¯ = ä¿ç•™ç»´åº¦
4. é€—å· = åˆ†éš”ä¸åŒè¾“å…¥

ä¾‹å­ï¼š'bik,bkj->bij'
     â†“   â†“    â†“
     A   B    C
     
- b: batchï¼ˆä¿ç•™ï¼‰
- i: è¡Œï¼ˆä¿ç•™ï¼‰
- j: åˆ—ï¼ˆä¿ç•™ï¼‰
- k: é‡å¤ï¼ˆæ±‚å’Œï¼‰
```

**einsumæ˜¯å¼ºå¤§è€Œä¼˜é›…çš„å¼ é‡è¿ç®—å·¥å…·ï¼** ğŸ¯