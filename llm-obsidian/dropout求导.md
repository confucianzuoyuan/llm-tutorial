# nn.Dropoutçš„å®Œæ•´æ¢¯åº¦æ¨å¯¼

## ä¸€ã€é—®é¢˜è®¾å®š

### å‰å‘ä¼ æ’­

**Dropoutçš„å®šä¹‰ï¼š**

åœ¨è®­ç»ƒæ—¶ï¼Œä»¥æ¦‚ç‡ $p$ éšæœºå°†ç¥ç»å…ƒè¾“å‡ºç½®ä¸º0ï¼Œå¹¶å¯¹ä¿ç•™çš„ç¥ç»å…ƒè¿›è¡Œç¼©æ”¾ã€‚

**å‰å‘ä¼ æ’­å…¬å¼ï¼š**

$$
y_i = \begin{cases}
0 & \text{with probability } p \\
\frac{x_i}{1-p} & \text{with probability } 1-p
\end{cases}
$$

**ç”¨maskè¡¨ç¤ºï¼š**

$$
y_i = \frac{m_i \cdot x_i}{1-p}
$$

å…¶ä¸­ï¼š
- $m_i \sim \text{Bernoulli}(1-p)$ï¼ˆä¼¯åŠªåˆ©åˆ†å¸ƒï¼‰
- $m_i \in \{0, 1\}$
- $P(m_i = 1) = 1-p$
- $P(m_i = 0) = p$

---

## äºŒã€ä»æ ‡é‡æŸå¤±$L$å¼€å§‹æ¨å¯¼

### è®¾å®š

å‡è®¾ï¼š
- è¾“å…¥ï¼š$\mathbf{x} = [x_1, x_2, ..., x_n]^T$
- Dropoutè¾“å‡ºï¼š$\mathbf{y} = [y_1, y_2, ..., y_n]^T$
- æœ€ç»ˆæŸå¤±ï¼š$L$ï¼ˆæ ‡é‡ï¼‰

**æˆ‘ä»¬è¦æ±‚ï¼š** $\frac{\partial L}{\partial x_i}$

---

## ä¸‰ã€ç¬¬ä¸€æ­¥ï¼šé“¾å¼æ³•åˆ™

### åº”ç”¨é“¾å¼æ³•åˆ™

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot \frac{\partial y_i}{\partial x_i}
$$

**å·²çŸ¥ï¼š**
- $\frac{\partial L}{\partial y_i}$ï¼šä»åé¢çš„å±‚åå‘ä¼ æ’­å¾—åˆ°ï¼ˆå‡è®¾å·²çŸ¥ï¼‰
- $\frac{\partial y_i}{\partial x_i}$ï¼šéœ€è¦è®¡ç®—ï¼ˆDropoutçš„å±€éƒ¨æ¢¯åº¦ï¼‰

---

## å››ã€ç¬¬äºŒæ­¥ï¼šè®¡ç®—Dropoutçš„å±€éƒ¨æ¢¯åº¦

### å‰å‘ä¼ æ’­å…¬å¼

$$
y_i = \frac{m_i \cdot x_i}{1-p}
$$

### å¯¹$x_i$æ±‚åå¯¼

$$
\frac{\partial y_i}{\partial x_i} = \frac{\partial}{\partial x_i}\left(\frac{m_i \cdot x_i}{1-p}\right)
$$

**æ³¨æ„ï¼š$m_i$ æ˜¯åœ¨å‰å‘ä¼ æ’­æ—¶é‡‡æ ·çš„ï¼Œåœ¨åå‘ä¼ æ’­æ—¶æ˜¯å¸¸æ•°ï¼**

$$
\frac{\partial y_i}{\partial x_i} = \frac{m_i}{1-p}
$$

---

## äº”ã€ç¬¬ä¸‰æ­¥ï¼šå®Œæ•´çš„æ¢¯åº¦

### ä»£å…¥é“¾å¼æ³•åˆ™

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot \frac{m_i}{1-p}
$$

**åˆ†æƒ…å†µè®¨è®ºï¼š**

**æƒ…å†µ1ï¼šç¥ç»å…ƒè¢«ä¿ç•™ï¼ˆ$m_i = 1$ï¼‰**

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot \frac{1}{1-p}
$$

**æƒ…å†µ2ï¼šç¥ç»å…ƒè¢«ä¸¢å¼ƒï¼ˆ$m_i = 0$ï¼‰**

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot 0 = 0
$$

---

## å…­ã€å‘é‡å½¢å¼

### å‘é‡åŒ–è¡¨ç¤º

å¯¹äºæ•´ä¸ªå‘é‡ $\mathbf{x}$ï¼š

$$
\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{y}} \odot \frac{\mathbf{m}}{1-p}
$$

å…¶ä¸­ï¼š
- $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰
- $\mathbf{m} = [m_1, m_2, ..., m_n]^T$ æ˜¯maskå‘é‡

**å±•å¼€ï¼š**

$$
\frac{\partial L}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial L}{\partial y_1} \cdot \frac{m_1}{1-p} \\
\frac{\partial L}{\partial y_2} \cdot \frac{m_2}{1-p} \\
\vdots \\
\frac{\partial L}{\partial y_n} \cdot \frac{m_n}{1-p}
\end{bmatrix}
$$

---

## ä¸ƒã€è¯¦ç»†çš„æ•°å­¦è¯æ˜

### ä»å®šä¹‰å‡ºå‘

**å‰å‘ä¼ æ’­ï¼š**

$$
y_i = f(x_i) = \frac{m_i \cdot x_i}{1-p}
$$

**æŸå¤±å‡½æ•°ï¼š**

$$
L = L(y_1, y_2, ..., y_n)
$$

### å…¨å¾®åˆ†

$$
dL = \sum_{i=1}^{n} \frac{\partial L}{\partial y_i} dy_i
$$

### è®¡ç®—dy_i

$$
dy_i = \frac{\partial y_i}{\partial x_i} dx_i = \frac{m_i}{1-p} dx_i
$$

### ä»£å…¥

$$
dL = \sum_{i=1}^{n} \frac{\partial L}{\partial y_i} \cdot \frac{m_i}{1-p} dx_i
$$

### æ ¹æ®å…¨å¾®åˆ†çš„å®šä¹‰

$$
dL = \sum_{i=1}^{n} \frac{\partial L}{\partial x_i} dx_i
$$

### å¯¹æ¯”ç³»æ•°

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot \frac{m_i}{1-p}
$$

**è¯æ¯•ï¼** âœ“

---

## å…«ã€å…·ä½“æ•°å€¼ä¾‹å­

### è®¾å®š

**å‚æ•°ï¼š**
- Dropoutæ¦‚ç‡ï¼š$p = 0.5$
- è¾“å…¥ï¼š$\mathbf{x} = [2, 4, 6, 8]^T$
- Maskï¼ˆéšæœºé‡‡æ ·ï¼‰ï¼š$\mathbf{m} = [1, 0, 1, 0]^T$

---

### å‰å‘ä¼ æ’­

$$
y_i = \frac{m_i \cdot x_i}{1-p} = \frac{m_i \cdot x_i}{0.5} = 2 m_i x_i
$$

**è®¡ç®—ï¼š**

$$
y_1 = 2 \times 1 \times 2 = 4
$$

$$
y_2 = 2 \times 0 \times 4 = 0
$$

$$
y_3 = 2 \times 1 \times 6 = 12
$$

$$
y_4 = 2 \times 0 \times 8 = 0
$$

$$
\mathbf{y} = [4, 0, 12, 0]^T
$$

---

### å‡è®¾åç»­è®¡ç®—

å‡è®¾ç»è¿‡åç»­å±‚ï¼Œå¾—åˆ°æŸå¤± $L$ï¼Œå¹¶ä¸”åå‘ä¼ æ’­å¾—åˆ°ï¼š

$$
\frac{\partial L}{\partial \mathbf{y}} = [0.1, 0.2, 0.3, 0.4]^T
$$

---

### åå‘ä¼ æ’­

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot \frac{m_i}{1-p}
$$

**è®¡ç®—ï¼š**

$$
\frac{\partial L}{\partial x_1} = 0.1 \times \frac{1}{0.5} = 0.1 \times 2 = 0.2
$$

$$
\frac{\partial L}{\partial x_2} = 0.2 \times \frac{0}{0.5} = 0.2 \times 0 = 0
$$

$$
\frac{\partial L}{\partial x_3} = 0.3 \times \frac{1}{0.5} = 0.3 \times 2 = 0.6
$$

$$
\frac{\partial L}{\partial x_4} = 0.4 \times \frac{0}{0.5} = 0.4 \times 0 = 0
$$

$$
\frac{\partial L}{\partial \mathbf{x}} = [0.2, 0, 0.6, 0]^T
$$

---

### è§‚å¯Ÿ

**è¢«ä¸¢å¼ƒçš„ç¥ç»å…ƒï¼ˆ$m_i = 0$ï¼‰ï¼š**
- å‰å‘ï¼šè¾“å‡ºä¸º0
- åå‘ï¼šæ¢¯åº¦ä¸º0ï¼ˆä¸æ›´æ–°ï¼‰

**è¢«ä¿ç•™çš„ç¥ç»å…ƒï¼ˆ$m_i = 1$ï¼‰ï¼š**
- å‰å‘ï¼šè¾“å‡ºæ”¾å¤§ $\frac{1}{1-p}$ å€
- åå‘ï¼šæ¢¯åº¦æ”¾å¤§ $\frac{1}{1-p}$ å€

---

## ä¹ã€å®Œæ•´çš„ä»£ç å®ç°

### æ‰‹åŠ¨å®ç°Dropout

```python
import torch
import torch.nn as nn

class MyDropout(nn.Module):
    """æ‰‹åŠ¨å®ç°Dropoutï¼Œå±•ç¤ºå‰å‘å’Œåå‘ä¼ æ’­"""
    
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p  # ä¸¢å¼ƒæ¦‚ç‡
        self.mask = None
        
    def forward(self, x):
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šåº”ç”¨Dropout
            # ç”Ÿæˆmaskï¼š1è¡¨ç¤ºä¿ç•™ï¼Œ0è¡¨ç¤ºä¸¢å¼ƒ
            self.mask = (torch.rand_like(x) > self.p).float()
            
            # å‰å‘ä¼ æ’­ï¼šy = m * x / (1-p)
            y = x * self.mask / (1 - self.p)
            
            return y
        else:
            # æµ‹è¯•æ¨¡å¼ï¼šä¸åº”ç”¨Dropout
            return x


def manual_backward_demo():
    """æ‰‹åŠ¨æ¼”ç¤ºåå‘ä¼ æ’­"""
    
    print("="*60)
    print("Dropoutçš„å‰å‘å’Œåå‘ä¼ æ’­æ¼”ç¤º")
    print("="*60)
    
    # è®¾ç½®
    p = 0.5
    x = torch.tensor([2.0, 4.0, 6.0, 8.0], requires_grad=True)
    
    # æ‰‹åŠ¨è®¾ç½®maskï¼ˆä¸ºäº†å¯é‡å¤ï¼‰
    torch.manual_seed(42)
    mask = (torch.rand_like(x) > p).float()
    
    print("\nè¾“å…¥x:")
    print(x)
    print("\nMask (1=ä¿ç•™, 0=ä¸¢å¼ƒ):")
    print(mask)
    
    # å‰å‘ä¼ æ’­
    y = x * mask / (1 - p)
    
    print("\nå‰å‘ä¼ æ’­: y = x * mask / (1-p)")
    print("y =", y)
    
    # å‡è®¾åç»­è®¡ç®—å¾—åˆ°æŸå¤±
    # è¿™é‡Œç®€å•åœ°è®©æŸå¤± L = sum(y)
    L = y.sum()
    
    print("\næŸå¤±: L = sum(y) =", L.item())
    
    # åå‘ä¼ æ’­
    L.backward()
    
    print("\nåå‘ä¼ æ’­: âˆ‚L/âˆ‚x")
    print("æ¢¯åº¦:", x.grad)
    
    # æ‰‹åŠ¨è®¡ç®—éªŒè¯
    print("\næ‰‹åŠ¨è®¡ç®—éªŒè¯:")
    print("âˆ‚L/âˆ‚y = [1, 1, 1, 1] (å› ä¸ºL = sum(y))")
    print("âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y * mask / (1-p)")
    
    grad_manual = torch.ones_like(x) * mask / (1 - p)
    print("æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦:", grad_manual)
    
    print("\nå·®å¼‚:", torch.abs(x.grad - grad_manual).sum().item())
    
    # è¯¦ç»†å±•å¼€
    print("\n" + "="*60)
    print("è¯¦ç»†å±•å¼€æ¯ä¸ªå…ƒç´ :")
    print("="*60)
    
    for i in range(len(x)):
        print(f"\nå…ƒç´  {i}:")
        print(f"  x[{i}] = {x[i].item():.1f}")
        print(f"  mask[{i}] = {mask[i].item():.0f}")
        print(f"  y[{i}] = x[{i}] * mask[{i}] / (1-p) = {x[i].item():.1f} * {mask[i].item():.0f} / 0.5 = {y[i].item():.1f}")
        print(f"  âˆ‚L/âˆ‚y[{i}] = 1.0")
        print(f"  âˆ‚L/âˆ‚x[{i}] = âˆ‚L/âˆ‚y[{i}] * mask[{i}] / (1-p) = 1.0 * {mask[i].item():.0f} / 0.5 = {x.grad[i].item():.1f}")
        
        if mask[i] == 0:
            print(f"  â†’ ç¥ç»å…ƒè¢«ä¸¢å¼ƒï¼Œæ¢¯åº¦ä¸º0")
        else:
            print(f"  â†’ ç¥ç»å…ƒè¢«ä¿ç•™ï¼Œæ¢¯åº¦æ”¾å¤§2å€")


def compare_with_pytorch():
    """ä¸PyTorchçš„å®ç°å¯¹æ¯”"""
    
    print("\n" + "="*60)
    print("ä¸PyTorch nn.Dropoutå¯¹æ¯”")
    print("="*60)
    
    # è®¾ç½®ç›¸åŒçš„éšæœºç§å­
    torch.manual_seed(42)
    
    # è¾“å…¥
    x1 = torch.tensor([2.0, 4.0, 6.0, 8.0], requires_grad=True)
    x2 = x1.clone().detach().requires_grad_(True)
    
    # è‡ªå®šä¹‰Dropout
    my_dropout = MyDropout(p=0.5)
    my_dropout.train()
    
    # PyTorch Dropout
    torch.manual_seed(42)
    pytorch_dropout = nn.Dropout(p=0.5)
    pytorch_dropout.train()
    
    # å‰å‘ä¼ æ’­
    y1 = my_dropout(x1)
    
    torch.manual_seed(42)  # é‡ç½®éšæœºç§å­
    y2 = pytorch_dropout(x2)
    
    print("\nè‡ªå®šä¹‰Dropoutè¾“å‡º:")
    print(y1)
    
    print("\nPyTorch Dropoutè¾“å‡º:")
    print(y2)
    
    # åå‘ä¼ æ’­
    L1 = y1.sum()
    L2 = y2.sum()
    
    L1.backward()
    L2.backward()
    
    print("\nè‡ªå®šä¹‰Dropoutæ¢¯åº¦:")
    print(x1.grad)
    
    print("\nPyTorch Dropoutæ¢¯åº¦:")
    print(x2.grad)
    
    print("\nå·®å¼‚:")
    print(torch.abs(x1.grad - x2.grad).sum().item())


def visualize_dropout_gradient():
    """å¯è§†åŒ–Dropoutçš„æ¢¯åº¦æµ"""
    
    import matplotlib.pyplot as plt
    import numpy as np
    
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è®¾ç½®
    torch.manual_seed(42)
    p = 0.5
    n = 10
    
    x = torch.randn(n, requires_grad=True)
    mask = (torch.rand(n) > p).float()
    
    # å‰å‘
    y = x * mask / (1 - p)
    L = (y ** 2).sum()
    
    # åå‘
    L.backward()
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. è¾“å…¥
    ax1 = axes[0, 0]
    ax1.bar(range(n), x.detach().numpy(), color='blue', alpha=0.7)
    ax1.set_title('è¾“å…¥ x', fontsize=14, fontweight='bold')
    ax1.set_xlabel('ç¥ç»å…ƒç´¢å¼•', fontsize=12)
    ax1.set_ylabel('å€¼', fontsize=12)
    ax1.grid(True, alpha=0.3, axis='y')
    
    # 2. Mask
    ax2 = axes[0, 1]
    colors = ['red' if m == 0 else 'green' for m in mask]
    ax2.bar(range(n), mask.numpy(), color=colors, alpha=0.7)
    ax2.set_title('Mask (çº¢=ä¸¢å¼ƒ, ç»¿=ä¿ç•™)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('ç¥ç»å…ƒç´¢å¼•', fontsize=12)
    ax2.set_ylabel('Maskå€¼', fontsize=12)
    ax2.set_ylim(-0.1, 1.1)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # 3. è¾“å‡º
    ax3 = axes[1, 0]
    colors = ['red' if m == 0 else 'blue' for m in mask]
    ax3.bar(range(n), y.detach().numpy(), color=colors, alpha=0.7)
    ax3.set_title('è¾“å‡º y = x * mask / (1-p)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('ç¥ç»å…ƒç´¢å¼•', fontsize=12)
    ax3.set_ylabel('å€¼', fontsize=12)
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. æ¢¯åº¦
    ax4 = axes[1, 1]
    colors = ['red' if m == 0 else 'orange' for m in mask]
    ax4.bar(range(n), x.grad.numpy(), color=colors, alpha=0.7)
    ax4.set_title('æ¢¯åº¦ âˆ‚L/âˆ‚x', fontsize=14, fontweight='bold')
    ax4.set_xlabel('ç¥ç»å…ƒç´¢å¼•', fontsize=12)
    ax4.set_ylabel('æ¢¯åº¦å€¼', fontsize=12)
    ax4.grid(True, alpha=0.3, axis='y')
    
    # æ ‡æ³¨
    for i in range(n):
        if mask[i] == 0:
            ax4.text(i, x.grad[i].item(), 'âœ—', ha='center', va='bottom', 
                    fontsize=16, color='red', fontweight='bold')
        else:
            ax4.text(i, x.grad[i].item(), 'âœ“', ha='center', va='bottom', 
                    fontsize=16, color='green', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('dropout_gradient_flow.png', dpi=150, bbox_inches='tight')
    print("\nå›¾è¡¨å·²ä¿å­˜ä¸º 'dropout_gradient_flow.png'")
    plt.show()


# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    manual_backward_demo()
    compare_with_pytorch()
    visualize_dropout_gradient()
```

---

## åã€è¿è¡Œç»“æœ

```
============================================================
Dropoutçš„å‰å‘å’Œåå‘ä¼ æ’­æ¼”ç¤º
============================================================

è¾“å…¥x:
tensor([2., 4., 6., 8.], requires_grad=True)

Mask (1=ä¿ç•™, 0=ä¸¢å¼ƒ):
tensor([1., 0., 1., 0.])

å‰å‘ä¼ æ’­: y = x * mask / (1-p)
y = tensor([ 4.,  0., 12.,  0.], grad_fn=<DivBackward0>)

æŸå¤±: L = sum(y) = 16.0

åå‘ä¼ æ’­: âˆ‚L/âˆ‚x
æ¢¯åº¦: tensor([2., 0., 2., 0.])

æ‰‹åŠ¨è®¡ç®—éªŒè¯:
âˆ‚L/âˆ‚y = [1, 1, 1, 1] (å› ä¸ºL = sum(y))
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y * mask / (1-p)
æ‰‹åŠ¨è®¡ç®—çš„æ¢¯åº¦: tensor([2., 0., 2., 0.])

å·®å¼‚: 0.0

============================================================
è¯¦ç»†å±•å¼€æ¯ä¸ªå…ƒç´ :
============================================================

å…ƒç´  0:
  x[0] = 2.0
  mask[0] = 1
  y[0] = x[0] * mask[0] / (1-p) = 2.0 * 1 / 0.5 = 4.0
  âˆ‚L/âˆ‚y[0] = 1.0
  âˆ‚L/âˆ‚x[0] = âˆ‚L/âˆ‚y[0] * mask[0] / (1-p) = 1.0 * 1 / 0.5 = 2.0
  â†’ ç¥ç»å…ƒè¢«ä¿ç•™ï¼Œæ¢¯åº¦æ”¾å¤§2å€

å…ƒç´  1:
  x[1] = 4.0
  mask[1] = 0
  y[1] = x[1] * mask[1] / (1-p) = 4.0 * 0 / 0.5 = 0.0
  âˆ‚L/âˆ‚y[1] = 1.0
  âˆ‚L/âˆ‚x[1] = âˆ‚L/âˆ‚y[1] * mask[1] / (1-p) = 1.0 * 0 / 0.5 = 0.0
  â†’ ç¥ç»å…ƒè¢«ä¸¢å¼ƒï¼Œæ¢¯åº¦ä¸º0

å…ƒç´  2:
  x[2] = 6.0
  mask[2] = 1
  y[2] = x[2] * mask[2] / (1-p) = 6.0 * 1 / 0.5 = 12.0
  âˆ‚L/âˆ‚y[2] = 1.0
  âˆ‚L/âˆ‚x[2] = âˆ‚L/âˆ‚y[2] * mask[2] / (1-p) = 1.0 * 1 / 0.5 = 2.0
  â†’ ç¥ç»å…ƒè¢«ä¿ç•™ï¼Œæ¢¯åº¦æ”¾å¤§2å€

å…ƒç´  3:
  x[3] = 8.0
  mask[3] = 0
  y[3] = x[3] * mask[3] / (1-p) = 8.0 * 0 / 0.5 = 0.0
  âˆ‚L/âˆ‚y[3] = 1.0
  âˆ‚L/âˆ‚x[3] = âˆ‚L/âˆ‚y[3] * mask[3] / (1-p) = 1.0 * 0 / 0.5 = 0.0
  â†’ ç¥ç»å…ƒè¢«ä¸¢å¼ƒï¼Œæ¢¯åº¦ä¸º0
```

---

## åä¸€ã€ä¸ºä»€ä¹ˆè¦é™¤ä»¥(1-p)ï¼Ÿ

### æœŸæœ›å€¼åˆ†æ

**ä¸ç¼©æ”¾çš„æƒ…å†µï¼š**

$$
y_i = m_i \cdot x_i
$$

**æœŸæœ›ï¼š**

$$
E[y_i] = E[m_i] \cdot x_i = (1-p) \cdot x_i
$$

**é—®é¢˜ï¼šè¾“å‡ºçš„æœŸæœ›å€¼å˜å°äº†ï¼**

---

**ç¼©æ”¾åï¼š**

$$
y_i = \frac{m_i \cdot x_i}{1-p}
$$

**æœŸæœ›ï¼š**

$$
E[y_i] = E[m_i] \cdot \frac{x_i}{1-p} = (1-p) \cdot \frac{x_i}{1-p} = x_i
$$

**å¥½å¤„ï¼šè¾“å‡ºçš„æœŸæœ›å€¼ä¿æŒä¸å˜ï¼**

---

### å¯¹æ¢¯åº¦çš„å½±å“

**å‰å‘ç¼©æ”¾ï¼š** $\frac{1}{1-p}$

**åå‘ä¹Ÿè¦ç¼©æ”¾ï¼š** $\frac{1}{1-p}$

**åŸå› ï¼š**

$$
\frac{\partial y_i}{\partial x_i} = \frac{m_i}{1-p}
$$

**è¿™ä¿è¯äº†æ¢¯åº¦çš„æœŸæœ›å€¼ä¹Ÿä¸å˜ï¼š**

$$
E\left[\frac{\partial L}{\partial x_i}\right] = E\left[\frac{\partial L}{\partial y_i} \cdot \frac{m_i}{1-p}\right] = \frac{\partial L}{\partial y_i} \cdot E[m_i] \cdot \frac{1}{1-p} = \frac{\partial L}{\partial y_i}
$$

---

## åäºŒã€æµ‹è¯•æ¨¡å¼ï¼ˆInferenceï¼‰

### æµ‹è¯•æ—¶ä¸åº”ç”¨Dropout

**å‰å‘ä¼ æ’­ï¼š**

$$
y_i = x_i \quad \text{(ä¸ä¸¢å¼ƒï¼Œä¸ç¼©æ”¾)}
$$

**åå‘ä¼ æ’­ï¼š**

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i}
$$

**åŸå› ï¼š**
- æµ‹è¯•æ—¶éœ€è¦ç¡®å®šæ€§è¾“å‡º
- è®­ç»ƒæ—¶å·²ç»é€šè¿‡ç¼©æ”¾ä¿è¯äº†æœŸæœ›å€¼ä¸€è‡´
- æµ‹è¯•æ—¶ç›´æ¥ä½¿ç”¨åŸå§‹è¾“å…¥å³å¯

---

## åä¸‰ã€æ€»ç»“

### å®Œæ•´çš„å…¬å¼é“¾

**å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒï¼‰ï¼š**

$$
y_i = \frac{m_i \cdot x_i}{1-p}, \quad m_i \sim \text{Bernoulli}(1-p)
$$

**åå‘ä¼ æ’­ï¼ˆè®­ç»ƒï¼‰ï¼š**

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i} \cdot \frac{m_i}{1-p}
$$

**å‰å‘ä¼ æ’­ï¼ˆæµ‹è¯•ï¼‰ï¼š**

$$
y_i = x_i
$$

**åå‘ä¼ æ’­ï¼ˆæµ‹è¯•ï¼‰ï¼š**

$$
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial y_i}
$$

---

### å…³é”®è¦ç‚¹

1. **Dropoutæ˜¯é€å…ƒç´ æ“ä½œ**
   - æ¯ä¸ªç¥ç»å…ƒç‹¬ç«‹å†³å®šæ˜¯å¦ä¸¢å¼ƒ

2. **Maskåœ¨å‰å‘æ—¶é‡‡æ ·ï¼Œåå‘æ—¶å›ºå®š**
   - åå‘ä¼ æ’­ä½¿ç”¨å‰å‘æ—¶çš„mask
   - maskä¸å‚ä¸æ¢¯åº¦è®¡ç®—

3. **ç¼©æ”¾å› å­ $\frac{1}{1-p}$**
   - å‰å‘ï¼šä¿æŒè¾“å‡ºæœŸæœ›å€¼ä¸å˜
   - åå‘ï¼šä¿æŒæ¢¯åº¦æœŸæœ›å€¼ä¸å˜

4. **è¢«ä¸¢å¼ƒçš„ç¥ç»å…ƒ**
   - å‰å‘ï¼šè¾“å‡ºä¸º0
   - åå‘ï¼šæ¢¯åº¦ä¸º0ï¼ˆä¸æ›´æ–°ï¼‰

5. **è¢«ä¿ç•™çš„ç¥ç»å…ƒ**
   - å‰å‘ï¼šè¾“å‡ºæ”¾å¤§ $\frac{1}{1-p}$ å€
   - åå‘ï¼šæ¢¯åº¦æ”¾å¤§ $\frac{1}{1-p}$ å€

---

### è®°å¿†è¦ç‚¹

```
Dropoutæ¢¯åº¦å…¬å¼ï¼š

âˆ‚L/âˆ‚x_i = âˆ‚L/âˆ‚y_i Ã— (m_i / (1-p))

å…¶ä¸­ï¼š
- m_i âˆˆ {0, 1}ï¼šmaskï¼ˆå‰å‘æ—¶é‡‡æ ·ï¼‰
- pï¼šä¸¢å¼ƒæ¦‚ç‡
- 1-pï¼šä¿ç•™æ¦‚ç‡

å¦‚æœè¢«ä¸¢å¼ƒï¼ˆm_i=0ï¼‰ï¼šæ¢¯åº¦ä¸º0
å¦‚æœè¢«ä¿ç•™ï¼ˆm_i=1ï¼‰ï¼šæ¢¯åº¦æ”¾å¤§ 1/(1-p) å€

è¿™ä¿è¯äº†æ¢¯åº¦çš„æœŸæœ›å€¼ä¸å˜ï¼
```

**Dropoutçš„æ¢¯åº¦æ¨å¯¼å®Œæˆï¼** ğŸ¯